{"cells":[{"cell_type":"code","source":["import sys\nimport csv\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import format_string\nfrom pyspark.sql.functions  import date_format\nspark = SparkSession.builder.appName(\"app\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#data =spark.read.format('csv').options(header='true',inferschema='true').load(\"/FileStore/tables/testFile.csv\")\ndata =spark.read.format('csv').options(header='true',inferschema='true').option(\"timestamp\", \"MM/dd/yyyy HH:mm:ss\").load(\"/FileStore/tables/data.csv\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data= data.withColumnRenamed('Primary Type','PType')\ndata= data.withColumnRenamed('Case Number','CNum')\ndata= data.withColumnRenamed('Location Description','LocDes')\ndata= data.withColumnRenamed('Community Area','CommArea')\ndata= data.withColumnRenamed('FBI Code','FBICode')\ndata= data.withColumnRenamed('X Coordinate','Xcoord')\ndata= data.withColumnRenamed('Y Coordinate','Ycoord')\ndata= data.withColumnRenamed('Updated On','UpdatedOn')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["data.createOrReplaceTempView('data')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["groupByDate = spark.sql('select YEAR(Date) as year,count(*) as cnt from data group by YEAR(Date)').collect()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["for row in groupByDate:\n  print('{0},{1}'.format(row['year'],row['cnt']))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["spark.sql('select YEAR(Date),count(*) from data group by YEAR(Date)').collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["data.createOrReplaceTempView('data')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["distCdTpDes = spark.sql('select distinct Iucr,PType,Description from data order by Iucr').collect()\nfor row in distCdTpDes:\n  print('{0},{1},{2}'.format(row['Iucr'],row['PType'],row['Description']))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["distCdTpDes= spark.sql('select Iucr,PType,Description,count(*) as co from data group by Iucr,PType,Description order by Iucr').collect()\nfor row in distCdTpDes:\n  print('{0},{1},{2},{3}'.format(row['Iucr'],row['PType'],row['Description'],row['co']))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["groupByDay = spark.sql('select DAY(Date) as days,count(*) as cnt from data group by DAY(Date)').collect()\nfor row in groupByDay:\n  print('{0},{1}'.format(row['days'],row['cnt']))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["groupByMonth = spark.sql('select MONTH(Date) as mon,count(*) as cnt from data group by MONTH(Date)').collect()\nfor row in groupByMonth:\n  print('{0},{1}'.format(row['mon'],row['cnt']))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["countPerDist = spark.sql('select District,count(*) as distCnt from data group by District').collect()\nfor row in countPerDist:\n  print('{0},{1}'.format(row['District'], row['distCnt']))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["countPerDistPerYr = spark.sql('select District,YEAR(Date) as yr,count(*) as distCnt from data group by District,YEAR(Date)').collect()\nfor row in countPerDistPerYr:\n  print('{0},{1},{2}'.format(row['District'], row['yr'],row['distCnt']))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["arrestPerOffense = spark.sql('select PType,count(*) as arrestCount from data where Arrest = TRUE group by PType order by PType').collect()\nfor row in arrestPerOffense:\n  print('{0},{1}'.format(row['PType'],row['arrestCount']))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#data.columns\nspark.sql('select distinct Description from data').collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["crimePerYear = spark.sql('select YEAR(Date) as Year,PType,count(*) as crimecount from data group by PType,YEAR(Date) order by PType').collect()\nfor row in crimePerYear:\n  print('{0},{1},{2}'.format(row['Year'],row['PType'],row['crimecount']))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["sex_assault = spark.sql('select count(*) as sex_assault_cnt, YEAR(Date) as year,Description from data where PType = \"CRIM SEXUAL ASSAULT\" group by YEAR(Date),Description').collect()\n\nfor row in sex_assault:\n  print('{0},{1},{2}'.format(row['sex_assault_cnt'],row['year'],row['Description']))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#chceking duplicate counts\n#spark.sql('SELECT ID, COUNT(ID) FROM data GROUP BY ID HAVING COUNT(ID)>1').collect()\nimport csv\ndic = {}\ncheck_block = spark.sql('select Block,count(*) as location_count from data where Latitude is NULL group by Block').collect()\nfor row in check_block:\n  dic[row['Block']] = row['location_count']\n\nwith open('mycsvfile.csv', 'w') as f:  # Just use 'w' mode in 3.x\n    w = csv.DictWriter(f, dic.keys())\n    w.writeheader()\n    mycsvfile.writerow(dic)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["spark.sql('select distinct PType,IUCR from data').collect()"],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"analysis1","notebookId":3525102900837167},"nbformat":4,"nbformat_minor":0}
